{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição de funções necessárias\n",
    "\n",
    "def get_noticias(url, keywords):\n",
    "    \"\"\"\n",
    "    Função que coleta notícias de uma página específica baseada em palavras-chave nos links.\n",
    "    Também coleta a data de publicação, se disponível.\n",
    "    \"\"\"\n",
    "    noticias = []\n",
    "    try:\n",
    "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        response.encoding = response.apparent_encoding  # Ajuste para decodificação correta\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Encontrar todos os links na página\n",
    "        links = soup.find_all('a', href=True)\n",
    "\n",
    "        # Filtrar e armazenar apenas os links que contêm as palavras-chave\n",
    "        for link_tag in links:\n",
    "            link = link_tag['href']\n",
    "            title = link_tag.get_text(strip=True)\n",
    "            \n",
    "            # Verificar se a palavra-chave está no link ou no título\n",
    "            if any(keyword in title.lower() for keyword in keywords):\n",
    "                # Se o link não é absoluto, fazê-lo absoluto\n",
    "                if not link.startswith('http'):\n",
    "                    link = urljoin(url, link)\n",
    "\n",
    "                # Tentar capturar a data de publicação da notícia\n",
    "                date = 'Data não disponível'\n",
    "                try:\n",
    "                    date_tag = soup.find('time')  # Se houver uma tag <time> na página\n",
    "                    if date_tag and date_tag.has_attr('datetime'):\n",
    "                        date = date_tag['datetime']\n",
    "                    else:\n",
    "                        # Tentativa alternativa, por exemplo, se a data estiver em um <span> com uma classe específica\n",
    "                        date_tag = soup.find('span', class_='data-publicacao')\n",
    "                        date = date_tag.get_text(strip=True) if date_tag else 'Data não disponível'\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                # Adicionar a notícia à lista, incluindo a data\n",
    "                if title:\n",
    "                    noticias.append({\"title\": title, \"link\": link, \"date\": date})\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erro ao acessar {url}: {e}\")\n",
    "    \n",
    "    return noticias\n",
    "\n",
    "def crawl_site(url, keywords, depth=3):\n",
    "    \"\"\"\n",
    "    Função recursiva para percorrer todo o site, coletando notícias relevantes.\n",
    "    Agora coleta a data da notícia.\n",
    "    \"\"\"\n",
    "    visited_links = set()\n",
    "    noticias = []\n",
    "\n",
    "    def _crawl(url, current_depth):\n",
    "        # Verificar se o link já foi visitado ou se a profundidade máxima foi atingida\n",
    "        if current_depth == 0 or url in visited_links:\n",
    "            return\n",
    "        \n",
    "        visited_links.add(url)  # Marcar o link como visitado\n",
    "        noticias_atual = get_noticias(url, keywords)\n",
    "\n",
    "        # Remover links duplicados antes de adicionar à lista de notícias\n",
    "        for noticia in noticias_atual:\n",
    "            if noticia[\"link\"] not in {n[\"link\"] for n in noticias}:\n",
    "                noticias.append(noticia)\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            response.encoding = response.apparent_encoding\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "            # Encontrar todos os links na página e seguir\n",
    "            links = soup.find_all('a', href=True)\n",
    "\n",
    "            for link_tag in links:\n",
    "                link = link_tag['href']\n",
    "                if not link.startswith('http'):\n",
    "                    link = urljoin(url, link)\n",
    "\n",
    "                # Verificar se o link é interno e não foi visitado\n",
    "                if url in link and link not in visited_links:\n",
    "                    _crawl(link, current_depth - 1)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Erro ao acessar {url}: {e}\")\n",
    "\n",
    "    _crawl(url, depth)\n",
    "    return noticias\n",
    "\n",
    "def get_text(url):\n",
    "    '''\n",
    "    Função \n",
    "    '''\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.encoding = response.apparent_encoding\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        for script in soup(['script', 'style']):\n",
    "            script.extract()\n",
    "\n",
    "        text = soup.get_text()\n",
    "\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split())\n",
    "\n",
    "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "        return text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao obter texto de {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def coluna_texto(df):\n",
    "    if 'link' in df:\n",
    "        df['corpo_texto'] = df['link'].apply(get_text)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "def extrair_para_df(data_dict, keywords):\n",
    "    base_url = data_dict['base_url']\n",
    "    depth = data_dict['depth']\n",
    "    \n",
    "    # Coletar notícias do site inteiro, seguindo links internos\n",
    "    df_nome = pd.DataFrame(crawl_site(base_url, keywords, depth=depth))\n",
    "    \n",
    "    # Remover títulos duplicados\n",
    "    df_nome = df_nome.drop_duplicates(subset='title')\n",
    "    \n",
    "    # Adicionar a coluna 'full_text' com o texto completo extraído de cada link\n",
    "    coluna_texto(df_nome)\n",
    "    \n",
    "    return df_nome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palavras-chave a serem buscadas\n",
    "keywords = [\"vacina\", \"vacinação\", \"vacinacao\", \"covid\", \"pandemia\", \"corona\", \"coronavírus\", \"coronavirus\", \"quarentena\", \"sars-cov-2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição dos dados a serem extraídos\n",
    "true_data = {\n",
    "    'g1 saude':{\n",
    "        'base_url':'https://g1.globo.com/saude/',\n",
    "        'depth':4\n",
    "    },\n",
    "    'gov saude':{\n",
    "        'base_url':'https://www.gov.br/saude/pt-br',\n",
    "        'depth':3\n",
    "    }\n",
    "}\n",
    "\n",
    "fake_data = {\n",
    "    'fato ou fake g1':{\n",
    "        'base_url':'https://g1.globo.com/fato-ou-fake/',\n",
    "        'depth':6\n",
    "    },\n",
    "    'aos fatos':{\n",
    "        'base_url':'https://www.aosfatos.org',\n",
    "        'depth':3\n",
    "    },\n",
    "    'gov fake':{\n",
    "        'base_url':'https://www.saude.gov.br/fakenews',\n",
    "        'depth':3\n",
    "    },\n",
    "    'boatos':{\n",
    "        'base_url':'https://www.boatos.org',\n",
    "        'depth':3\n",
    "    },\n",
    "    'mpv':{\n",
    "        'base_url':'https://medicospelavidacovid19.com.br/',\n",
    "        'depth':3\n",
    "    }    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: 1/5 (fake datasets)\n",
      "Done: 2/5 (fake datasets)\n",
      "Done: 3/5 (fake datasets)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alice.ferreira/anaconda3/lib/python3.12/html/parser.py:171: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: 4/5 (fake datasets)\n",
      "Erro ao obter texto de mailto:contato@medicospelavidacovid19.com.br: No connection adapters were found for 'mailto:contato@medicospelavidacovid19.com.br'\n",
      "Done: 5/5 (fake datasets)\n",
      "Done: 1/2 (true datasets)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao obter texto de https://www.gov.br/saude/pt-br/assuntos/pcdt/arquivos/2022/portal-portaria-gm-2-994-lciam-e-protocolo-de-sca-1.pdf: The markup you provided was rejected by the parser. Trying a different parser or a different encoding may help.\n",
      "\n",
      "Original exception(s) from parser:\n",
      " AssertionError: expected name token at '<![���aFe\\x0b�\\x0b�g�\\x1d\\x15�W�'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao obter texto de http://%20https//www.gov.br/saude/pt-br/coronavirus/vacinas/plano-nacional-de-operacionalizacao-da-vacina-contra-a-covid-19: HTTPConnectionPool(host='%20https', port=80): Max retries exceeded with url: /www.gov.br/saude/pt-br/coronavirus/vacinas/plano-nacional-de-operacionalizacao-da-vacina-contra-a-covid-19 (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x7f77f5aba990>: Failed to resolve '%20https' ([Errno -2] Name or service not known)\"))\n",
      "Erro ao obter texto de https://registra-rh-covid19.saude.gov.br/: HTTPSConnectionPool(host='registra-rh-covid19.saude.gov.br', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f77f8fec4a0>: Failed to resolve 'registra-rh-covid19.saude.gov.br' ([Errno -2] Name or service not known)\"))\n",
      "Erro ao obter texto de https:// https//antigo.saude.gov.br/images/pdf/2020/marco/04/Calendario-Vacinao-2020-Crian--a.pdf: HTTPSConnectionPool(host='%20https', port=443): Max retries exceeded with url: /antigo.saude.gov.br/images/pdf/2020/marco/04/Calendario-Vacinao-2020-Crian--a.pdf (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f77f8da69f0>: Failed to resolve '%20https' ([Errno -2] Name or service not known)\"))\n",
      "Done: 2/2 (true datasets)\n"
     ]
    }
   ],
   "source": [
    "# Extrai dados da internet e cria uma lista com os dfs (falsos e verdadeiros)\n",
    "fake_dfs = []\n",
    "n = 0\n",
    "for data_dict in fake_data.values():\n",
    "    fake_dfs.append(extrair_para_df(data_dict, keywords))\n",
    "    n += 1\n",
    "    print(f'Done: {n}/5 (fake datasets)')\n",
    "    \n",
    "true_dfs = []\n",
    "n = 0\n",
    "for data_dict in true_data.values():\n",
    "    true_dfs.append(extrair_para_df(data_dict, keywords))\n",
    "    n += 1\n",
    "    print(f'Done: {n}/2 (true datasets)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devido à estrutura desse site, precisamos mudar a sintaxe para o www.e-farsas.com\n",
    "base_url = \"https://www.e-farsas.com/secoes/verdadeiro-2/page\"\n",
    "\n",
    "# Coletar notícias de todas as páginas do site, seguindo links internos\n",
    "df_efarsa_verdadeiro = []\n",
    "for i in range(1, 159):  # Número de páginas a ser percorrido\n",
    "    page_url = f\"{base_url}{i}\"\n",
    "    df_efarsa_verdadeiro.append(extrair_para_df({'base_url':page_url,'depth':3}, keywords))\n",
    "\n",
    "df_efarsa_verdadeiro =  pd.concat(df_efarsa_verdadeiro, ignore_index=True)\n",
    "true_dfs.append(df_efarsa_verdadeiro)\n",
    "\n",
    "\n",
    "base_url = \"https://www.e-farsas.com/secoes/falso-2/page\"\n",
    "\n",
    "df_efarsa_falso = []\n",
    "for i in range(1, 159):\n",
    "    page_url = f\"{base_url}{i}\"\n",
    "    df_efarsa_falso.append(extrair_para_df({'base_url':page_url,'depth':3}, keywords))\n",
    "\n",
    "df_efarsa_falso =  pd.concat(df_efarsa_falso, ignore_index=True)\n",
    "fake_dfs.append(df_efarsa_falso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>date</th>\n",
       "      <th>corpo_texto</th>\n",
       "      <th>classe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>https://g1.globo.com/saude/coronavirus/</td>\n",
       "      <td>Data não disponível</td>\n",
       "      <td>g1 Coronavirus: Tudo sobre o COVID-19 Coronaví...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lucraram com desinformação sobre a pandemia</td>\n",
       "      <td>https://www.aosfatos.org/noticias/como-sete-si...</td>\n",
       "      <td>Data não disponível</td>\n",
       "      <td>Como sete sites lucraram com anúncios no Googl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Covid-19</td>\n",
       "      <td>https://www.gov.br/saude/pt-br/assuntos/covid-19</td>\n",
       "      <td>Data não disponível</td>\n",
       "      <td>Covid-19 — Ministério da Saúde Ir para o Conte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vacinação</td>\n",
       "      <td>https://www.gov.br/saude/pt-br/vacinacao</td>\n",
       "      <td>Data não disponível</td>\n",
       "      <td>Vacinação — Ministério da Saúde Ir para o Cont...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Calendário de Vacinação</td>\n",
       "      <td>https://www.gov.br/saude/pt-br/vacinacao/calen...</td>\n",
       "      <td>Data não disponível</td>\n",
       "      <td>Calendário de Vacinação — Ministério da Saúde ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>Uma jovem de 17 anos morreu por causa da vacin...</td>\n",
       "      <td>http://www.e-farsas.com/uma-jovem-de-17-anos-m...</td>\n",
       "      <td>Data não disponível</td>\n",
       "      <td>Uma jovem de 17 anos morreu por causa da vacin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>A vacina contra a febre amarela é um veneno mo...</td>\n",
       "      <td>http://www.e-farsas.com/vacina-contra-febre-am...</td>\n",
       "      <td>Data não disponível</td>\n",
       "      <td>A vacina contra a febre amarela é um veneno mo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Vacina contra a rubéola foi a causa da microce...</td>\n",
       "      <td>http://www.e-farsas.com/vacina-contra-a-rubeol...</td>\n",
       "      <td>Data não disponível</td>\n",
       "      <td>Vacina contra a rubéola foi a causa da microce...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>Vacina anticâncer rins e pele! Noticia boa (se...</td>\n",
       "      <td>http://www.e-farsas.com/vacina-anticancer-rins...</td>\n",
       "      <td>Data não disponível</td>\n",
       "      <td>Vacina anticâncer rins e pele! Noticia boa (se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>Não tome a Vacina!</td>\n",
       "      <td>http://www.e-farsas.com/nao-tome-a-vacina.html</td>\n",
       "      <td>Data não disponível</td>\n",
       "      <td>Vacina da gripe possui muito mercúrio e é mort...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0                                          Coronavirus   \n",
       "1          lucraram com desinformação sobre a pandemia   \n",
       "2                                             Covid-19   \n",
       "3                                            Vacinação   \n",
       "4                              Calendário de Vacinação   \n",
       "..                                                 ...   \n",
       "235  Uma jovem de 17 anos morreu por causa da vacin...   \n",
       "236  A vacina contra a febre amarela é um veneno mo...   \n",
       "237  Vacina contra a rubéola foi a causa da microce...   \n",
       "238  Vacina anticâncer rins e pele! Noticia boa (se...   \n",
       "239                                 Não tome a Vacina!   \n",
       "\n",
       "                                                  link                 date  \\\n",
       "0              https://g1.globo.com/saude/coronavirus/  Data não disponível   \n",
       "1    https://www.aosfatos.org/noticias/como-sete-si...  Data não disponível   \n",
       "2     https://www.gov.br/saude/pt-br/assuntos/covid-19  Data não disponível   \n",
       "3             https://www.gov.br/saude/pt-br/vacinacao  Data não disponível   \n",
       "4    https://www.gov.br/saude/pt-br/vacinacao/calen...  Data não disponível   \n",
       "..                                                 ...                  ...   \n",
       "235  http://www.e-farsas.com/uma-jovem-de-17-anos-m...  Data não disponível   \n",
       "236  http://www.e-farsas.com/vacina-contra-febre-am...  Data não disponível   \n",
       "237  http://www.e-farsas.com/vacina-contra-a-rubeol...  Data não disponível   \n",
       "238  http://www.e-farsas.com/vacina-anticancer-rins...  Data não disponível   \n",
       "239     http://www.e-farsas.com/nao-tome-a-vacina.html  Data não disponível   \n",
       "\n",
       "                                           corpo_texto  classe  \n",
       "0    g1 Coronavirus: Tudo sobre o COVID-19 Coronaví...       0  \n",
       "1    Como sete sites lucraram com anúncios no Googl...       0  \n",
       "2    Covid-19 — Ministério da Saúde Ir para o Conte...       0  \n",
       "3    Vacinação — Ministério da Saúde Ir para o Cont...       0  \n",
       "4    Calendário de Vacinação — Ministério da Saúde ...       0  \n",
       "..                                                 ...     ...  \n",
       "235  Uma jovem de 17 anos morreu por causa da vacin...       0  \n",
       "236  A vacina contra a febre amarela é um veneno mo...       0  \n",
       "237  Vacina contra a rubéola foi a causa da microce...       0  \n",
       "238  Vacina anticâncer rins e pele! Noticia boa (se...       0  \n",
       "239  Vacina da gripe possui muito mercúrio e é mort...       0  \n",
       "\n",
       "[240 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>date</th>\n",
       "      <th>corpo_texto</th>\n",
       "      <th>classe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>https://g1.globo.com/saude/coronavirus/</td>\n",
       "      <td>Data não disponível</td>\n",
       "      <td>g1 Coronavirus: Tudo sobre o COVID-19 Coronaví...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Novas doses da vacina contra mpox chegam este ...</td>\n",
       "      <td>https://g1.globo.com/saude/noticia/2024/11/12/...</td>\n",
       "      <td>Data não disponível</td>\n",
       "      <td>Novas doses da vacina contra mpox chegam este ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VÍDEO: Agnaldo Rayol cantou 'Ave Maria' e 'Pai...</td>\n",
       "      <td>https://g1.globo.com/sp/sao-paulo/noticia/2024...</td>\n",
       "      <td>Data não disponível</td>\n",
       "      <td>VÍDEO: Agnaldo Rayol cantou 'Ave Maria' e 'Pai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Após afetar Tóquio, atletas testam positivo pa...</td>\n",
       "      <td>https://g1.globo.com/mundo/olimpiadas/paris-20...</td>\n",
       "      <td>Data não disponível</td>\n",
       "      <td>Após afetar Tóquio, atletas testam positivo pa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O que muda na campanha de vacinação contra cov...</td>\n",
       "      <td>https://g1.globo.com/saude/noticia/2024/06/01/...</td>\n",
       "      <td>Data não disponível</td>\n",
       "      <td>O que muda na campanha de vacinação contra cov...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>Governo do Maranhão vem fornecendo kits com cl...</td>\n",
       "      <td>http://www.e-farsas.com/governo-do-maranhao-ve...</td>\n",
       "      <td>Data não disponível</td>\n",
       "      <td>Governo do Maranhão vem fornecendo kits com cl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>“Kit COVID-19” para combater o novo coronavíru...</td>\n",
       "      <td>http://www.e-farsas.com/kit-covid-19-para-comb...</td>\n",
       "      <td>Data não disponível</td>\n",
       "      <td>\"Kit COVID-19\" para combater o novo coronavíru...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>Autoridades, parem de distorcer fatos sobre a ...</td>\n",
       "      <td>http://www.e-farsas.com/autoridades-parem-de-d...</td>\n",
       "      <td>Data não disponível</td>\n",
       "      <td>Autoridades, parem de distorcer fatos sobre a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>O livro “A realidade de Madhu” de 2013 previu ...</td>\n",
       "      <td>http://www.e-farsas.com/o-livro-a-realidade-de...</td>\n",
       "      <td>Data não disponível</td>\n",
       "      <td>O livro “A realidade de Madhu” de 2013 previu ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>É verdade que um defensor do movimento antivac...</td>\n",
       "      <td>http://www.e-farsas.com/e-verdade-que-um-defen...</td>\n",
       "      <td>Data não disponível</td>\n",
       "      <td>É verdade que um defensor do movimento antivac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>256 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0                                          Coronavirus   \n",
       "1    Novas doses da vacina contra mpox chegam este ...   \n",
       "2    VÍDEO: Agnaldo Rayol cantou 'Ave Maria' e 'Pai...   \n",
       "3    Após afetar Tóquio, atletas testam positivo pa...   \n",
       "4    O que muda na campanha de vacinação contra cov...   \n",
       "..                                                 ...   \n",
       "251  Governo do Maranhão vem fornecendo kits com cl...   \n",
       "252  “Kit COVID-19” para combater o novo coronavíru...   \n",
       "253  Autoridades, parem de distorcer fatos sobre a ...   \n",
       "254  O livro “A realidade de Madhu” de 2013 previu ...   \n",
       "255  É verdade que um defensor do movimento antivac...   \n",
       "\n",
       "                                                  link                 date  \\\n",
       "0              https://g1.globo.com/saude/coronavirus/  Data não disponível   \n",
       "1    https://g1.globo.com/saude/noticia/2024/11/12/...  Data não disponível   \n",
       "2    https://g1.globo.com/sp/sao-paulo/noticia/2024...  Data não disponível   \n",
       "3    https://g1.globo.com/mundo/olimpiadas/paris-20...  Data não disponível   \n",
       "4    https://g1.globo.com/saude/noticia/2024/06/01/...  Data não disponível   \n",
       "..                                                 ...                  ...   \n",
       "251  http://www.e-farsas.com/governo-do-maranhao-ve...  Data não disponível   \n",
       "252  http://www.e-farsas.com/kit-covid-19-para-comb...  Data não disponível   \n",
       "253  http://www.e-farsas.com/autoridades-parem-de-d...  Data não disponível   \n",
       "254  http://www.e-farsas.com/o-livro-a-realidade-de...  Data não disponível   \n",
       "255  http://www.e-farsas.com/e-verdade-que-um-defen...  Data não disponível   \n",
       "\n",
       "                                           corpo_texto  classe  \n",
       "0    g1 Coronavirus: Tudo sobre o COVID-19 Coronaví...       1  \n",
       "1    Novas doses da vacina contra mpox chegam este ...       1  \n",
       "2    VÍDEO: Agnaldo Rayol cantou 'Ave Maria' e 'Pai...       1  \n",
       "3    Após afetar Tóquio, atletas testam positivo pa...       1  \n",
       "4    O que muda na campanha de vacinação contra cov...       1  \n",
       "..                                                 ...     ...  \n",
       "251  Governo do Maranhão vem fornecendo kits com cl...       1  \n",
       "252  \"Kit COVID-19\" para combater o novo coronavíru...       1  \n",
       "253  Autoridades, parem de distorcer fatos sobre a ...       1  \n",
       "254  O livro “A realidade de Madhu” de 2013 previu ...       1  \n",
       "255  É verdade que um defensor do movimento antivac...       1  \n",
       "\n",
       "[256 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Junta os dfs fakes e salva\n",
    "df_combinedfake = pd.concat(fake_dfs, ignore_index=True)\n",
    "df_combinedfake['classe'] = 0\n",
    "df_combinedfake.to_csv(\"dados/df_combinedfake.csv\", index=False, encoding='utf-8')\n",
    "display(df_combinedfake)\n",
    "\n",
    "# Junta os verdadeiros e salva\n",
    "df_combinedtrue = pd.concat(true_dfs, ignore_index=True)\n",
    "df_combinedtrue['classe'] = 1\n",
    "df_combinedtrue.to_csv(\"dados/df_combinedtrue.csv\", index=False, encoding='utf-8')\n",
    "display(df_combinedtrue)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
