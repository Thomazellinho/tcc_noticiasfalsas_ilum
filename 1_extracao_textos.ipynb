{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição de funções necessárias\n",
    "\n",
    "def get_noticias(url, keywords):\n",
    "    \"\"\"\n",
    "    Função que coleta notícias de uma página específica baseada em palavras-chave nos links.\n",
    "    Também coleta a data de publicação, se disponível.\n",
    "    \"\"\"\n",
    "    noticias = []\n",
    "    try:\n",
    "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        response.encoding = response.apparent_encoding  # Ajuste para decodificação correta\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Encontrar todos os links na página\n",
    "        links = soup.find_all('a', href=True)\n",
    "\n",
    "        # Filtrar e armazenar apenas os links que contêm as palavras-chave\n",
    "        for link_tag in links:\n",
    "            link = link_tag['href']\n",
    "            title = link_tag.get_text(strip=True)\n",
    "            \n",
    "            # Verificar se a palavra-chave está no link ou no título\n",
    "            if any(keyword in title.lower() for keyword in keywords):\n",
    "                # Se o link não é absoluto, fazê-lo absoluto\n",
    "                if not link.startswith('http'):\n",
    "                    link = urljoin(url, link)\n",
    "\n",
    "                # Tentar capturar a data de publicação da notícia\n",
    "                date = 'Data não disponível'\n",
    "                try:\n",
    "                    date_tag = soup.find('time')  # Se houver uma tag <time> na página\n",
    "                    if date_tag and date_tag.has_attr('datetime'):\n",
    "                        date = date_tag['datetime']\n",
    "                    else:\n",
    "                        # Tentativa alternativa, por exemplo, se a data estiver em um <span> com uma classe específica\n",
    "                        date_tag = soup.find('span', class_='data-publicacao')\n",
    "                        date = date_tag.get_text(strip=True) if date_tag else 'Data não disponível'\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                # Adicionar a notícia à lista, incluindo a data\n",
    "                if title:\n",
    "                    noticias.append({\"title\": title, \"link\": link, \"date\": date})\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erro ao acessar {url}: {e}\")\n",
    "    \n",
    "    return noticias\n",
    "\n",
    "def crawl_site(url, keywords, depth=3):\n",
    "    \"\"\"\n",
    "    Função recursiva para percorrer todo o site, coletando notícias relevantes.\n",
    "    Agora coleta a data da notícia.\n",
    "    \"\"\"\n",
    "    visited_links = set()\n",
    "    noticias = []\n",
    "\n",
    "    def _crawl(url, current_depth):\n",
    "        # Verificar se o link já foi visitado ou se a profundidade máxima foi atingida\n",
    "        if current_depth == 0 or url in visited_links:\n",
    "            return\n",
    "        \n",
    "        visited_links.add(url)  # Marcar o link como visitado\n",
    "        noticias_atual = get_noticias(url, keywords)\n",
    "\n",
    "        # Remover links duplicados antes de adicionar à lista de notícias\n",
    "        for noticia in noticias_atual:\n",
    "            if noticia[\"link\"] not in {n[\"link\"] for n in noticias}:\n",
    "                noticias.append(noticia)\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            response.encoding = response.apparent_encoding\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "            # Encontrar todos os links na página e seguir\n",
    "            links = soup.find_all('a', href=True)\n",
    "\n",
    "            for link_tag in links:\n",
    "                link = link_tag['href']\n",
    "                if not link.startswith('http'):\n",
    "                    link = urljoin(url, link)\n",
    "\n",
    "                # Verificar se o link é interno e não foi visitado\n",
    "                if url in link and link not in visited_links:\n",
    "                    _crawl(link, current_depth - 1)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Erro ao acessar {url}: {e}\")\n",
    "\n",
    "    _crawl(url, depth)\n",
    "    return noticias\n",
    "\n",
    "def get_text(url):\n",
    "    '''\n",
    "    Função \n",
    "    '''\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.encoding = response.apparent_encoding\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        for script in soup(['script', 'style']):\n",
    "            script.extract()\n",
    "\n",
    "        text = soup.get_text()\n",
    "\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split())\n",
    "\n",
    "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "        return text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao obter texto de {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def coluna_texto(df):\n",
    "    if 'link' in df:\n",
    "        df['corpo_texto'] = df['link'].apply(get_text)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "def extrair_para_df(data_dict, keywords):\n",
    "    base_url = data_dict['base_url']\n",
    "    depth = data_dict['depth']\n",
    "    \n",
    "    # Coletar notícias do site inteiro, seguindo links internos\n",
    "    df_nome = pd.DataFrame(crawl_site(base_url, keywords, depth=depth))\n",
    "    \n",
    "    # Remover títulos duplicados\n",
    "    df_nome = df_nome.drop_duplicates(subset='title')\n",
    "    \n",
    "    # Adicionar a coluna 'full_text' com o texto completo extraído de cada link\n",
    "    coluna_texto(df_nome)\n",
    "    \n",
    "    return df_nome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palavras-chave a serem buscadas\n",
    "keywords = [\"vacina\", \"vacinação\", \"vacinacao\", \"covid\", \"pandemia\", \"corona\", \"coronavírus\", \"coronavirus\", \"quarentena\", \"sars-cov-2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição dos dados a serem extraídos\n",
    "true_data = {\n",
    "    'g1 saude':{\n",
    "        'base_url':'https://g1.globo.com/saude/',\n",
    "        'depth':4\n",
    "    },\n",
    "    'gov saude':{\n",
    "        'base_url':'https://www.gov.br/saude/pt-br',\n",
    "        'depth':3\n",
    "    }\n",
    "}\n",
    "\n",
    "fake_data = {\n",
    "    'fato ou fake g1':{\n",
    "        'base_url':'https://g1.globo.com/fato-ou-fake/',\n",
    "        'depth':6\n",
    "    },\n",
    "    'aos fatos':{\n",
    "        'base_url':'https://www.aosfatos.org',\n",
    "        'depth':3\n",
    "    },\n",
    "    'gov fake':{\n",
    "        'base_url':'https://www.saude.gov.br/fakenews',\n",
    "        'depth':3\n",
    "    },\n",
    "    'boatos':{\n",
    "        'base_url':'https://www.boatos.org',\n",
    "        'depth':3\n",
    "    },\n",
    "    'mpv':{\n",
    "        'base_url':'https://medicospelavidacovid19.com.br/',\n",
    "        'depth':5\n",
    "    }    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrai dados da internet e cria uma lista com os dfs (falsos e verdadeiros)\n",
    "fake_dfs = []\n",
    "for data_dict in fake_data.values():\n",
    "    fake_dfs.append(extrair_para_df(data_dict, keywords))\n",
    "    \n",
    "true_dfs = []\n",
    "for data_dict in true_data.values():\n",
    "    true_dfs.append(extrair_para_df(data_dict, keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devido à estrutura desse site, precisamos mudar a sintaxe para o www.e-farsas.com\n",
    "base_url = \"https://www.e-farsas.com/secoes/verdadeiro-2/page\"\n",
    "\n",
    "# Coletar notícias de todas as páginas do site, seguindo links internos\n",
    "df_efarsa_verdadeiro = []\n",
    "for i in range(1, 159):  # Número de páginas a ser percorrido\n",
    "    page_url = f\"{base_url}{i}\"\n",
    "    df_efarsa_verdadeiro.append(extrair_para_df({'base_url':page_url,'depth':3}, keywords))\n",
    "\n",
    "df_efarsa_verdadeiro =  pd.concat(df_efarsa_verdadeiro, ignore_index=True)\n",
    "true_dfs.append(df_efarsa_verdadeiro)\n",
    "\n",
    "\n",
    "base_url = \"https://www.e-farsas.com/secoes/falso-2/page\"\n",
    "\n",
    "df_efarsa_falso = []\n",
    "for i in range(1, 159):\n",
    "    page_url = f\"{base_url}{i}\"\n",
    "    df_efarsa_falso.append(extrair_para_df({'base_url':page_url,'depth':3}, keywords))\n",
    "\n",
    "df_efarsa_falso =  pd.concat(df_efarsa_falso, ignore_index=True)\n",
    "fake_dfs.append(df_efarsa_falso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Junta os dfs fakes e salva\n",
    "df_combinedfake = pd.concat(fake_dfs, ignore_index=True)\n",
    "df_combinedfake['classe'] = 0\n",
    "df_combinedfake.to_csv(\"dados/df_combinedfake.csv\", index=False, encoding='utf-8')\n",
    "display(df_combinedfake)\n",
    "\n",
    "# Junta os verdadeiros e salva\n",
    "df_combinedtrue = pd.concat(true_dfs, ignore_index=True)\n",
    "df_combinedfake['classe'] = 1\n",
    "df_combinedtrue.to_csv(\"dados/df_combinedtrue.csv\", index=False, encoding='utf-8')\n",
    "display(df_combinedtrue)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
