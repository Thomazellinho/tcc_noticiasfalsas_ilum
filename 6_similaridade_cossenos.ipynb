{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações\n",
    "\n",
    "import nltk\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    cohen_kappa_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "import ast\n",
    "\n",
    "# Cross validate\n",
    "\n",
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "\n",
    "# Modelos\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Similaridade de Cossenos\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>date</th>\n",
       "      <th>corpo_texto</th>\n",
       "      <th>noticia_falsa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[verdade, casos, mortes, covid19, diminuíram, ...</td>\n",
       "      <td>http://www.e-farsas.com/e-verdade-que-os-casos...</td>\n",
       "      <td>Data não disponível</td>\n",
       "      <td>[verdade, casos, mortes, covid19, diminuíram, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[estudo, recomenda, uso, máscara, contra, covi...</td>\n",
       "      <td>https://projetocomprova.com.br/publicações/est...</td>\n",
       "      <td>2020/11/06</td>\n",
       "      <td>[postagem, facebook, fonte, estudar, verdade, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[covid, vai, parar, infectar, maioria, pessoas...</td>\n",
       "      <td>https://www.boatos.org/saude/covid-parar-quand...</td>\n",
       "      <td>26/11/2020</td>\n",
       "      <td>[explicar, milésimo, vir, enfiar, cabeça, pess...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[considerações, críticas, vacinas, contra, cov...</td>\n",
       "      <td>https://medicospelavidacovid19.com.br/geral/co...</td>\n",
       "      <td>Data não disponível</td>\n",
       "      <td>[considerações, críticas, vacinas, contra, cov...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[janssen, fará, reunião, dia, 16, pedir, uso, ...</td>\n",
       "      <td>https://noticias.uol.com.br/ultimas-noticias/r...</td>\n",
       "      <td>04/03/2021 22h21</td>\n",
       "      <td>[janssen, subsidiário, johnson, johnson, pedir...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3330</th>\n",
       "      <td>[cidade, lourenço, zerou, internação, covid, g...</td>\n",
       "      <td>https://www.e-farsas.com/a-cidade-de-sao-loure...</td>\n",
       "      <td>16/03/2021</td>\n",
       "      <td>[publicação, espalhar, rede, social, whatsapp,...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3331</th>\n",
       "      <td>[esquemas, vacinaisacesse, esquemas, vacinais,...</td>\n",
       "      <td>https://www.gov.br/saude/pt-br/assuntos/covid-...</td>\n",
       "      <td>Data não disponível</td>\n",
       "      <td>[esquemas, vacinais, ministério, saúde, ir, co...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3332</th>\n",
       "      <td>[saúde, autoriza, 45, leitos, uti, covid19, se...</td>\n",
       "      <td>https://www.gov.br/saude/pt-br/assuntos/notici...</td>\n",
       "      <td>14/04/2021 17h57</td>\n",
       "      <td>[ministério, saudar, autorizar, n, quartafeira...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3333</th>\n",
       "      <td>[fake, foto, mostre, coração, pessoa, após, us...</td>\n",
       "      <td>https://g1.globo.com/fato-ou-fake/coronavirus/...</td>\n",
       "      <td>01/09/2020</td>\n",
       "      <td>[circular, rede, social, foto, coração, predom...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3334</th>\n",
       "      <td>[maduro, suspende, toque, recolher, fronteiriç...</td>\n",
       "      <td>https://g1.globo.com/bemestar/coronavirus/noti...</td>\n",
       "      <td>29/11/2020 21h24</td>\n",
       "      <td>[presidente, venezuelano, nicolás, madurar, an...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3335 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0     [verdade, casos, mortes, covid19, diminuíram, ...   \n",
       "1     [estudo, recomenda, uso, máscara, contra, covi...   \n",
       "2     [covid, vai, parar, infectar, maioria, pessoas...   \n",
       "3     [considerações, críticas, vacinas, contra, cov...   \n",
       "4     [janssen, fará, reunião, dia, 16, pedir, uso, ...   \n",
       "...                                                 ...   \n",
       "3330  [cidade, lourenço, zerou, internação, covid, g...   \n",
       "3331  [esquemas, vacinaisacesse, esquemas, vacinais,...   \n",
       "3332  [saúde, autoriza, 45, leitos, uti, covid19, se...   \n",
       "3333  [fake, foto, mostre, coração, pessoa, após, us...   \n",
       "3334  [maduro, suspende, toque, recolher, fronteiriç...   \n",
       "\n",
       "                                                   link                 date  \\\n",
       "0     http://www.e-farsas.com/e-verdade-que-os-casos...  Data não disponível   \n",
       "1     https://projetocomprova.com.br/publicações/est...           2020/11/06   \n",
       "2     https://www.boatos.org/saude/covid-parar-quand...           26/11/2020   \n",
       "3     https://medicospelavidacovid19.com.br/geral/co...  Data não disponível   \n",
       "4     https://noticias.uol.com.br/ultimas-noticias/r...    04/03/2021 22h21    \n",
       "...                                                 ...                  ...   \n",
       "3330  https://www.e-farsas.com/a-cidade-de-sao-loure...           16/03/2021   \n",
       "3331  https://www.gov.br/saude/pt-br/assuntos/covid-...  Data não disponível   \n",
       "3332  https://www.gov.br/saude/pt-br/assuntos/notici...     14/04/2021 17h57   \n",
       "3333  https://g1.globo.com/fato-ou-fake/coronavirus/...           01/09/2020   \n",
       "3334  https://g1.globo.com/bemestar/coronavirus/noti...    29/11/2020 21h24    \n",
       "\n",
       "                                            corpo_texto  noticia_falsa  \n",
       "0     [verdade, casos, mortes, covid19, diminuíram, ...            0.0  \n",
       "1     [postagem, facebook, fonte, estudar, verdade, ...            0.0  \n",
       "2     [explicar, milésimo, vir, enfiar, cabeça, pess...            0.0  \n",
       "3     [considerações, críticas, vacinas, contra, cov...            0.0  \n",
       "4     [janssen, subsidiário, johnson, johnson, pedir...            1.0  \n",
       "...                                                 ...            ...  \n",
       "3330  [publicação, espalhar, rede, social, whatsapp,...            0.0  \n",
       "3331  [esquemas, vacinais, ministério, saúde, ir, co...            1.0  \n",
       "3332  [ministério, saudar, autorizar, n, quartafeira...            1.0  \n",
       "3333  [circular, rede, social, foto, coração, predom...            0.0  \n",
       "3334  [presidente, venezuelano, nicolás, madurar, an...            1.0  \n",
       "\n",
       "[3335 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importação do Dataframe contendo títulos de notícias relacionadas a saúde.\n",
    "\n",
    "df = pd.read_csv('dados/df_final.zip')\n",
    "\n",
    "# Re-transforma as listas em listas\n",
    "df['title'] = df['title'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "df['corpo_texto'] = df['corpo_texto'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões de X: (3335, 6414)\n",
      "Dimensões de y: (3335,)\n"
     ]
    }
   ],
   "source": [
    "# Inicializa o vetorizador de TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Criação de vetores\n",
    "X = df['title'].apply(lambda x: ' '.join(x))\n",
    "y = df['noticia_falsa']\n",
    "\n",
    "X = tfidf_vectorizer.fit_transform(X, y)\n",
    "\n",
    "print(\"Dimensões de X:\", X.shape)\n",
    "print(\"Dimensões de y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define os modelos\n",
    "\n",
    "# Árvore de Decisão\n",
    "modelo_ad = DecisionTreeClassifier()\n",
    "\n",
    "# Floresta Aleatória\n",
    "modelo_rf = RandomForestClassifier()\n",
    "\n",
    "# Modelo Regressão Logística\n",
    "modelo_lr = LogisticRegression()\n",
    "\n",
    "# SVC\n",
    "# Parâmetros\n",
    "C = 77\n",
    "kernel = 'rbf'\n",
    "gamma = 0.01\n",
    "\n",
    "# Cria modelo\n",
    "modelo_svm = svm.SVC(C=C, kernel=kernel, gamma=gamma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "def trata_padrao(titulo, usar_links=False, link=None):\n",
    "    '''\n",
    "    Função para padronizar o tratamento dos títulos.\n",
    "\n",
    "    Args:\n",
    "    titulo (str) = título da notícia\n",
    "    usar_links (bool) = usar ou não o link\n",
    "    link (str) = link da notícia\n",
    "    '''\n",
    "\n",
    "    # Pré-processamento do titulo\n",
    "\n",
    "    titulo = str(titulo).lower()\n",
    "    titulo = nltk.word_tokenize(titulo)\n",
    "    titulo = [word for word in titulo if word not in stop_words]\n",
    "    titulo = [' '.join(titulo)]\n",
    "    temp_tit = titulo\n",
    "\n",
    "    # se usar link, pre processar link\n",
    "    if usar_links:\n",
    "        link = [split_and_clean(link)]\n",
    "        temp_link = [' '.join(i) for i in link]\n",
    "        print(temp_tit,temp_link)\n",
    "        temp_tit_link = [' '.join(list(a)) for a in zip(temp_tit, temp_link)]\n",
    "        processed = temp_tit_link\n",
    "    else:\n",
    "        processed = titulo\n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define modelos para teste\n",
    "modelos = [modelo_ad, modelo_rf, modelo_lr, modelo_svm]\n",
    "\n",
    "# Treina modelos com todos os dados\n",
    "for modelo in modelos:\n",
    "    modelo.fit(X, y)\n",
    "\n",
    "# Função para predizer novos títulos\n",
    "def predict_title(titulo):\n",
    "    preprocessed_title = trata_padrao(titulo)\n",
    "\n",
    "    tfidf_vector = tfidf_vectorizer.transform(preprocessed_title)\n",
    "    preds = dict()\n",
    "\n",
    "    for modelo in modelos:\n",
    "        preds[modelo] = modelo.predict(tfidf_vector)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparar(df):\n",
    "    '''\n",
    "    Função para comparar predições de diferentes modelos com o target\n",
    "\n",
    "    Args:\n",
    "        df (pandas.df): dataframe com coluna '1' sendo os títulos/corpus, e '2' com o target/label\n",
    "        \n",
    "    Returns:\n",
    "        list: Uma lista contendo 4 listas (uma para cada modelo: Decision Tree, Random Forest, Logistic Regression, SVC)\n",
    "    '''\n",
    "    DT, RF, LR, SVC = [], [], [], []\n",
    "    for index, row in df.iterrows():\n",
    "        text = row[0]\n",
    "        target = row[1]\n",
    "        pred = predict_title(text)\n",
    "        pred['Target'] = [target]\n",
    "        \n",
    "        # Guardar as previsões em suas respectivas listas\n",
    "        keys = [i for i in pred.keys()]\n",
    "        DT.append(pred[keys[0]][0])\n",
    "        RF.append(pred[keys[1]][0])\n",
    "        LR.append(pred[keys[2]][0])\n",
    "        SVC.append(pred[keys[3]][0])\n",
    "        \n",
    "        print(f'Notícia: {text}\\n')\n",
    "        for i in pred:\n",
    "            print(\"\\t{}\\t{}\".format(i, pred[i]))\n",
    "        print('-----------------------------------------------------')\n",
    "        print()\n",
    "        \n",
    "    return [DT, RF, LR, SVC]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similaridade de Cosseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de similaridade de cosseno entre dois textos\n",
    "def cosine_distance_countvectorizer_method(s1, s2):\n",
    "    allsentences = [s1, s2]\n",
    "    vectorizer = CountVectorizer()\n",
    "    all_sentences_to_vector = vectorizer.fit_transform(allsentences)\n",
    "    text_to_vector_v1 = all_sentences_to_vector.toarray()[0].tolist()\n",
    "    text_to_vector_v2 = all_sentences_to_vector.toarray()[1].tolist()\n",
    "\n",
    "    cosine = distance.cosine(text_to_vector_v1, text_to_vector_v2)\n",
    "    return round((1 - cosine) * 100, 2)  # Similaridade em %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função principal que faz tudo: calcula similaridade, chama predict_title e gera gráfico\n",
    "def process_title_and_predict(combined_df, palavra_especifica):\n",
    "    similaridades = []\n",
    "\n",
    "    # Calculando a similaridade da palavra específica com todas as outras\n",
    "    for i, title in enumerate(combined_df['title']):\n",
    "        if title != palavra_especifica:  # Não compara consigo mesma\n",
    "            similaridade = cosine_distance_countvectorizer_method(palavra_especifica, title)\n",
    "            noticia_falsa_status = combined_df.iloc[i]['noticia_falsa']  # Valor de 'noticia_falsa' correspondente\n",
    "            similaridades.append((title, similaridade, noticia_falsa_status))\n",
    "\n",
    "    # Ordenando as 10 palavras mais semelhantes\n",
    "    similaridades_ordenadas = sorted(similaridades, key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "    # Exibindo as 10 palavras mais semelhantes e o status de 'noticia_falsa'\n",
    "    print(f\"\\nAs 10 títulos mais semelhantes a '{palavra_especifica}':\")\n",
    "    for title, sim, noticia_falsa in similaridades_ordenadas:\n",
    "        print(f\"{title} - Similaridade: {sim}% - Notícia Falsa: {noticia_falsa}\")\n",
    "\n",
    "    # Gráfico de barras para similaridades\n",
    "    titles = [f'{title} - {noticia_falsa}' for title, sim, noticia_falsa in similaridades_ordenadas]\n",
    "    sims = [sim for title, sim, noticia_falsa in similaridades_ordenadas]\n",
    "\n",
    "    # Limpando o gráfico anterior\n",
    "    plt.clf()\n",
    "\n",
    "    # Plotando o gráfico\n",
    "    plt.barh(titles, sims, color='skyblue')\n",
    "    plt.xlabel('Similaridade (%)')\n",
    "    plt.ylabel('Títulos')\n",
    "    plt.title(f'Similaridade com: {palavra_especifica}')\n",
    "    plt.gca().invert_yaxis()  # Inverte a ordem dos títulos no eixo Y para ficar mais intuitivo\n",
    "    \n",
    "    # Exibir o gráfico no notebook ou ambiente gráfico\n",
    "    plt.show()\n",
    "\n",
    "    # Chamada da função de predição (predict_title)\n",
    "    print(f\"\\nPredição para o título: '{palavra_especifica}'\")\n",
    "    resultado_predicao = predict_title(palavra_especifica)\n",
    "    print(\"Resultado da Predição:\", resultado_predicao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m titulo_verdadeiro \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDigite o título de uma notícia verdadeira para encontrar similaridades e prever: \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m process_title_and_predict(df, titulo_verdadeiro)\n",
      "Cell \u001b[0;32mIn[24], line 8\u001b[0m, in \u001b[0;36mprocess_title_and_predict\u001b[0;34m(combined_df, palavra_especifica)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m i, title \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(combined_df[\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m      7\u001b[0m     \u001b[39mif\u001b[39;00m title \u001b[39m!=\u001b[39m palavra_especifica:  \u001b[39m# Não compara consigo mesma\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m         similaridade \u001b[39m=\u001b[39m cosine_distance_countvectorizer_method(palavra_especifica, title)\n\u001b[1;32m      9\u001b[0m         noticia_falsa_status \u001b[39m=\u001b[39m combined_df\u001b[39m.\u001b[39miloc[i][\u001b[39m'\u001b[39m\u001b[39mnoticia_falsa\u001b[39m\u001b[39m'\u001b[39m]  \u001b[39m# Valor de 'noticia_falsa' correspondente\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         similaridades\u001b[39m.\u001b[39mappend((title, similaridade, noticia_falsa_status))\n",
      "Cell \u001b[0;32mIn[23], line 5\u001b[0m, in \u001b[0;36mcosine_distance_countvectorizer_method\u001b[0;34m(s1, s2)\u001b[0m\n\u001b[1;32m      3\u001b[0m allsentences \u001b[39m=\u001b[39m [s1, s2]\n\u001b[1;32m      4\u001b[0m vectorizer \u001b[39m=\u001b[39m CountVectorizer()\n\u001b[0;32m----> 5\u001b[0m all_sentences_to_vector \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mfit_transform(allsentences)\n\u001b[1;32m      6\u001b[0m text_to_vector_v1 \u001b[39m=\u001b[39m all_sentences_to_vector\u001b[39m.\u001b[39mtoarray()[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m      7\u001b[0m text_to_vector_v2 \u001b[39m=\u001b[39m all_sentences_to_vector\u001b[39m.\u001b[39mtoarray()[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1364\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1365\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1366\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1367\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1368\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1369\u001b[0m             )\n\u001b[1;32m   1370\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1372\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_count_vocab(raw_documents, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfixed_vocabulary_)\n\u001b[1;32m   1374\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1375\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1259\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   1258\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1259\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1260\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1261\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:108\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 108\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:66\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[0;32m---> 66\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39mlower()\n\u001b[1;32m     67\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "titulo_verdadeiro = input(\"Digite o título de uma notícia verdadeira para encontrar similaridades e prever: \")\n",
    "\n",
    "process_title_and_predict(df, titulo_verdadeiro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titulo_falso = input(\"Digite o título de uma notícia falsa para encontrar similaridades e prever: \")\n",
    "\n",
    "process_title_and_predict(df, titulo_falso)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
