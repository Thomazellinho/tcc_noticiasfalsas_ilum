{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "import csv\n",
    "\n",
    "# Caso o NLTK não esteja configurado:\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa os dados extraídos\n",
    "df_true = pd.read_csv('dados/df_combinedtrue.csv')\n",
    "df_false = pd.read_csv('dados/df_combinedfake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o FakeRecogna\n",
    "df_fakerecogna = pd.read_excel('dados/FakeRecogna.xlsx')\n",
    "\n",
    "# Filtra apenas as notícias de saúde\n",
    "df_fakerecogna = df_fakerecogna.loc[df_fakerecogna[\"Categoria\"] == 'saúde']\n",
    "\n",
    "df_fakerecogna = df_fakerecogna.drop(['Categoria', 'Subtitulo', 'Autor'], axis=1)\n",
    "df_fakerecogna = df_fakerecogna[['Titulo', 'URL', 'Data', 'Classe', 'Noticia']]\n",
    "df_fakerecogna = df_fakerecogna.rename(columns = {'Titulo':'title', 'URL': 'link', 'Data': 'date', 'Classe':'classe','Noticia':'corpo_texto'})\n",
    "df_fakerecogna['classe'] = df_fakerecogna['classe'].astype('int64')\n",
    "\n",
    "display(df_fakerecogna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palavras a serem buscadas\n",
    "keywords = [\"vacina\", \"vacinação\", \"vacinacao\", \"covid\", \"pandemia\", \"corona\", \"coronavírus\", \"coronavirus\", \"quarentena\", \"sars-cov-2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# filtra o dataset com o mesmo critério que o de extração\n",
    "df_fakerecogna = df_fakerecogna[df_fakerecogna['title'].str.contains('|'.join(keywords), case=False, na=False)]\n",
    "\n",
    "display(df_fakerecogna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar os DataFrames\n",
    "df_final = pd.concat([df_true, df_false, df_fakerecogna], ignore_index=True)\n",
    "\n",
    "display(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- PRÉ PROCESSAMENTO ---------------------------- #\n",
    "# Garante que há conteúdo na coluna de título, corpo do texto e classe\n",
    "df_final = df_final.dropna(subset=['title', 'corpo_texto', 'classe'])\n",
    "\n",
    "# Garante que não há notícias repetidas\n",
    "df_final = df_final.drop_duplicates()\n",
    "df_final = df_final.reset_index(drop=True)\n",
    "\n",
    "# Remove maiúsculas\n",
    "df_final['corpo_texto'] = df_final['corpo_texto'].str.lower()\n",
    "df_final['title'] = df_final['title'].str.lower()\n",
    "\n",
    "# Tokeniza\n",
    "df_final['corpo_texto'] = df_final['corpo_texto'].apply(lambda x: nltk.word_tokenize(x, language='portuguese'))\n",
    "df_final['title'] = df_final['title'].apply(lambda x: nltk.word_tokenize(x, language='portuguese'))\n",
    "\n",
    "# Remove stop_words e pontuação\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "def clean_text(tokens):\n",
    "    return [re.sub(r'[^\\w\\s]', '', word) for word in tokens if word not in stop_words and re.sub(r'[^\\w\\s]', '', word)]\n",
    "\n",
    "df_final['corpo_texto'] = df_final['corpo_texto'].apply(clean_text)\n",
    "df_final['title'] = df_final['title'].apply(clean_text)\n",
    "\n",
    "# Embaralhar e resetar o índice\n",
    "df_final = shuffle(df_final).reset_index(drop=True)\n",
    "\n",
    "display(df_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar o df final\n",
    "csv_file_path = \"dados/df_final.csv\"\n",
    "\n",
    "compression_options = dict(method='zip', archive_name=csv_file_path)\n",
    "df_final.to_csv(f'dados/df_final.zip', index=False, encoding='utf-8', quoting=csv.QUOTE_ALL, escapechar='\\\\', compression=compression_options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
