{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "# Cross validation\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "\n",
    "# Modelos\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define funções essenciais\n",
    "\n",
    "# Variáveis para padronização das métricas\n",
    "metrics = {\n",
    "    \"accuracy\": accuracy_score,\n",
    "    \"precision\": precision_score,\n",
    "    \"roc_auc\": roc_auc_score\n",
    "}\n",
    "\n",
    "def evaluate_metrics(y_true, y_pred, y_pred_proba, metrics=metrics):\n",
    "    # Calculate scores\n",
    "    scores = {}\n",
    "    for name, metric in metrics.items():\n",
    "        # Handle metrics that require probabilities (like ROC AUC)\n",
    "        if name == 'roc_auc':\n",
    "            scores[name] = metric(y_true, y_pred_proba)\n",
    "        else:\n",
    "            scores[name] = metric(y_true, y_pred)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa Dataframe contendo títulos de notícias relacionadas a saúde\n",
    "df = pd.read_csv('dados/df_final.zip')\n",
    "\n",
    "# Re-transforma as listas em listas\n",
    "df['title'] = df['title'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "df['corpo_texto'] = df['corpo_texto'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa porção para validação\n",
    "df_main, df_validation = train_test_split(df, test_size=0.25, random_state=42, stratify=df['classe'])\n",
    "\n",
    "display(df_main, df_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checa distribuição de classes entre dados separados\n",
    "print(f'Distribuição para o df principal: {df_main['classe'].value_counts()[0]/\\\n",
    "      df_main['classe'].value_counts()[1]}')\n",
    "print(f'\\nDistribuição para o df de validação: {df_validation['classe'].value_counts()[0]/\\\n",
    "      df_validation['classe'].value_counts()[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa o vetorizador de TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Criação de vetores\n",
    "X = df_main['title'].apply(lambda x: ' '.join(x))\n",
    "y = df_main['classe']\n",
    "\n",
    "print(\"Dimensões de X:\", X.shape)\n",
    "print(\"Dimensões de y:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Árvore de Decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria modelo e define pipeline com TF-IDF\n",
    "modelo_ad = DecisionTreeClassifier()\n",
    "pipeline_ad = Pipeline([('TF-IDF', tfidf_vectorizer), ('modelo', modelo_ad)])\n",
    "\n",
    "# Validação cruzada\n",
    "resultados_ad = cross_validate(pipeline_ad, X, y, scoring=[score for score in metrics.keys()])\n",
    "\n",
    "acc_ad = resultados_ad['test_accuracy'].mean()\n",
    "acc_ad_std = np.std(resultados_ad['test_accuracy'])\n",
    "prec_ad = resultados_ad['test_precision'].mean()\n",
    "prec_ad_std = np.std(resultados_ad['test_precision'])\n",
    "rocauc_ad = resultados_ad['test_roc_auc'].mean()\n",
    "rocauc_ad_std = np.std(resultados_ad['test_roc_auc'])\n",
    "print(f'Acurácia: {acc_ad:.3f}±{acc_ad_std:.3f}\\\n",
    "    \\nPrecisão: {prec_ad:.3f}±{prec_ad_std:.3f}\\\n",
    "    \\nROC-AUC: {rocauc_ad:.3f}±{rocauc_ad_std:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floresta Aleatória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria modelo\n",
    "modelo_rf = RandomForestClassifier()\n",
    "pipeline_rf = Pipeline([('TF-IDF', tfidf_vectorizer), ('modelo', modelo_rf)])\n",
    "\n",
    "# Validação cruzada\n",
    "resultados_rf = cross_validate(pipeline_rf, X, y, scoring=[score for score in metrics.keys()])\n",
    "\n",
    "acc_rf = resultados_rf['test_accuracy'].mean()\n",
    "acc_rf_std = np.std(resultados_rf['test_accuracy'])\n",
    "prec_rf = resultados_rf['test_precision'].mean()\n",
    "prec_rf_std = np.std(resultados_rf['test_precision'])\n",
    "rocauc_rf = resultados_rf['test_roc_auc'].mean()\n",
    "rocauc_rf_std = np.std(resultados_rf['test_roc_auc'])\n",
    "print(f'Acurácia: {acc_rf:.3f}±{acc_rf_std:.3f}\\\n",
    "    \\nPrecisão: {prec_rf:.3f}±{prec_rf_std:.3f}\\\n",
    "    \\nROC-AUC: {rocauc_rf:.3f}±{rocauc_rf_std:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria modelo\n",
    "modelo_lr = LogisticRegression()\n",
    "pipeline_lr = Pipeline([('TF-IDF', tfidf_vectorizer), ('modelo', modelo_lr)])\n",
    "\n",
    "# Validação cruzada\n",
    "resultados_lr = cross_validate(pipeline_lr, X, y, scoring=[score for score in metrics.keys()])\n",
    "\n",
    "acc_lr = resultados_lr['test_accuracy'].mean()\n",
    "acc_lr_std = np.std(resultados_lr['test_accuracy'])\n",
    "prec_lr = resultados_lr['test_precision'].mean()\n",
    "prec_lr_std = np.std(resultados_lr['test_precision'])\n",
    "rocauc_lr = resultados_lr['test_roc_auc'].mean()\n",
    "rocauc_lr_std = np.std(resultados_lr['test_roc_auc'])\n",
    "print(f'Acurácia: {acc_lr:.3f}±{acc_lr_std:.3f}\\\n",
    "    \\nPrecisão: {prec_lr:.3f}±{prec_lr_std:.3f}\\\n",
    "    \\nROC-AUC: {rocauc_lr:.3f}±{rocauc_lr_std:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros\n",
    "C = 77\n",
    "kernel = 'rbf'\n",
    "gamma = 0.01\n",
    "\n",
    "# Cria modelo\n",
    "modelo_svm = svm.SVC(C=C, kernel=kernel, gamma=gamma, probability=True)\n",
    "pipeline_svm = Pipeline([('TF-IDF', tfidf_vectorizer), ('modelo', modelo_svm)])\n",
    "\n",
    "# Validação cruzada\n",
    "resultados_svm = cross_validate(pipeline_svm, X, y, scoring=[score for score in metrics.keys()])\n",
    "\n",
    "acc_svm = resultados_svm['test_accuracy'].mean()\n",
    "acc_svm_std = np.std(resultados_svm['test_accuracy'])\n",
    "prec_svm = resultados_svm['test_precision'].mean()\n",
    "prec_svm_std = np.std(resultados_svm['test_precision'])\n",
    "rocauc_svm = resultados_svm['test_roc_auc'].mean()\n",
    "rocauc_svm_std = np.std(resultados_svm['test_roc_auc'])\n",
    "print(f'Acurácia: {acc_svm:.3f}±{acc_svm_std:.3f}\\\n",
    "    \\nPrecisão: {prec_svm:.3f}±{prec_svm_std:.3f}\\\n",
    "    \\nROC-AUC: {rocauc_svm:.3f}±{rocauc_svm_std:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo os dados\n",
    "accuracy_list = [acc_ad, acc_rf, acc_lr, acc_svm]\n",
    "precision_list = [prec_ad, prec_rf, prec_lr, prec_svm]\n",
    "rocauc_list = [rocauc_ad, rocauc_rf, rocauc_lr, rocauc_svm]\n",
    "acc_std_list = [acc_ad_std, acc_rf_std, acc_lr_std, acc_svm_std]\n",
    "prec_std_list = [prec_ad_std, prec_rf_std, prec_lr_std, prec_svm_std]\n",
    "rocauc_std_list = [rocauc_ad_std, rocauc_rf_std, rocauc_lr_std, rocauc_svm_std]\n",
    "model_labels = ['Árvore de Decisão', 'Floresta Aleatória', 'Regressão Logística', 'SVM']\n",
    "colors = ['red', 'orange', 'pink', 'purple']\n",
    "\n",
    "# Posições das barras para os grupos\n",
    "x = np.arange(3)  # Precisão, Acurácia e ROC-AUC\n",
    "width = 0.15      # Largura das barras\n",
    "\n",
    "# Criando o gráfico sem zoom\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for i in range(len(model_labels)):\n",
    "    # Precisão\n",
    "    ax.bar(x[0] + i * width - (1.5 * width), precision_list[i], width, label=model_labels[i], color=colors[i])\n",
    "    ax.errorbar(x[0] + i * width - (1.5 * width), precision_list[i], yerr=prec_std_list[i], fmt='o', color='black', capsize=5)\n",
    "    # Acurácia\n",
    "    ax.bar(x[1] + i * width - (1.5 * width), accuracy_list[i], width, color=colors[i])\n",
    "    ax.errorbar(x[1] + i * width - (1.5 * width), accuracy_list[i], yerr=acc_std_list[i], fmt='o', color='black', capsize=5)\n",
    "    # ROC-AUC\n",
    "    ax.bar(x[2] + i * width - (1.5 * width), rocauc_list[i], width, color=colors[i])\n",
    "    ax.errorbar(x[2] + i * width - (1.5 * width), rocauc_list[i], yerr=rocauc_std_list[i], fmt='o', color='black', capsize=5)\n",
    "\n",
    "# Configurando os rótulos e título\n",
    "ax.set_xlabel('Métrica')\n",
    "ax.set_ylabel('Valores')\n",
    "ax.set_title('Precisão, Acurácia e ROC-AUC dos Modelos para os Títulos (Sem Zoom)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Precisão', 'Acurácia', 'ROC-AUC'])\n",
    "ax.legend()\n",
    "\n",
    "# Exibindo o gráfico sem zoom\n",
    "plt.show()\n",
    "\n",
    "# Criando o gráfico com zoom\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for i in range(len(model_labels)):\n",
    "    # Precisão\n",
    "    ax.bar(x[0] + i * width - (1.5 * width), precision_list[i], width, label=model_labels[i], color=colors[i])\n",
    "    ax.errorbar(x[0] + i * width - (1.5 * width), precision_list[i], yerr=prec_std_list[i], fmt='o', color='black', capsize=5)\n",
    "    # Acurácia\n",
    "    ax.bar(x[1] + i * width - (1.5 * width), accuracy_list[i], width, color=colors[i])\n",
    "    ax.errorbar(x[1] + i * width - (1.5 * width), accuracy_list[i], yerr=acc_std_list[i], fmt='o', color='black', capsize=5)\n",
    "    # ROC-AUC\n",
    "    ax.bar(x[2] + i * width - (1.5 * width), rocauc_list[i], width, color=colors[i])\n",
    "    ax.errorbar(x[2] + i * width - (1.5 * width), rocauc_list[i], yerr=rocauc_std_list[i], fmt='o', color='black', capsize=5)\n",
    "\n",
    "# Configurando os rótulos e título\n",
    "ax.set_xlabel('Métrica')\n",
    "ax.set_ylabel('Valores')\n",
    "ax.set_title('Precisão, Acurácia e ROC-AUC dos Modelos para os Títulos (Com Zoom)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Precisão', 'Acurácia', 'ROC-AUC'])\n",
    "ax.legend()\n",
    "\n",
    "# Definindo o limite do eixo y para dar o \"zoom\"\n",
    "plt.ylim(0.75, 1)\n",
    "\n",
    "# Exibindo o gráfico com zoom\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste com dados separados pra validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa o vetorizador de TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Criação de vetores\n",
    "X_val = df_validation['title'].apply(lambda x: ' '.join(x))\n",
    "y_val = df_validation['classe'].to_numpy()\n",
    "\n",
    "print(\"Dimensões de X:\", X_val.shape)\n",
    "print(\"Dimensões de y:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula score de cada método para df de validação\n",
    "\n",
    "pipelines = [pipeline_ad, pipeline_rf, pipeline_lr, pipeline_svm]\n",
    "\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "rocauc_list = []\n",
    "\n",
    "\n",
    "for pipeline in pipelines:\n",
    "    pipeline.fit(X,y)\n",
    "    y_val_pred = pipeline.predict(X_val)\n",
    "    y_val_pred_proba = pipeline.predict_proba(X_val)[:,1]\n",
    "    scores = evaluate_metrics(y_val, y_val_pred, y_val_pred_proba)\n",
    "    \n",
    "    accuracy_list.append(scores['accuracy'])\n",
    "    precision_list.append(scores['precision'])\n",
    "    rocauc_list.append(scores['roc_auc'])\n",
    "    \n",
    "    print(f'{str(pipeline[1])}:\\n    Acurácia: {scores['accuracy']:.3f}\\\n",
    "        \\n    Precisão: {scores['precision']:.3f}\\n    ROC-AUC: {scores['roc_auc']:.3f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positions for the groups of bars\n",
    "x = np.arange(3)  # One point for \"Precision\", \"Accuracy\", and \"ROC-AUC\"\n",
    "width = 0.15  # Width of the bars\n",
    "\n",
    "# Creating the plot without zoom\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for i in range(len(model_labels)):\n",
    "    # Precision\n",
    "    ax.bar(x[0] + i * width - (1.5 * width), precision_list[i], width, label=model_labels[i], color=colors[i])\n",
    "    # Accuracy\n",
    "    ax.bar(x[1] + i * width - (1.5 * width), accuracy_list[i], width, color=colors[i])\n",
    "    # ROC-AUC\n",
    "    ax.bar(x[2] + i * width - (1.5 * width), rocauc_list[i], width, color=colors[i])\n",
    "\n",
    "# Configuring labels and title\n",
    "ax.set_xlabel('Métrica')\n",
    "ax.set_ylabel('Valores')\n",
    "ax.set_title('Validação - Precisão, Acurácia e ROC-AUC dos Modelos para os Títulos (Sem Zoom)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Precisão', 'Acurácia', 'ROC-AUC'])\n",
    "ax.legend()\n",
    "\n",
    "# Displaying the plot without zoom\n",
    "plt.show()\n",
    "\n",
    "# Creating the plot with zoom\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for i in range(len(model_labels)):\n",
    "    # Precision\n",
    "    ax.bar(x[0] + i * width - (1.5 * width), precision_list[i], width, label=model_labels[i], color=colors[i])\n",
    "    # Accuracy\n",
    "    ax.bar(x[1] + i * width - (1.5 * width), accuracy_list[i], width, color=colors[i])\n",
    "    # ROC-AUC\n",
    "    ax.bar(x[2] + i * width - (1.5 * width), rocauc_list[i], width, color=colors[i])\n",
    "\n",
    "# Configuring labels and title\n",
    "ax.set_xlabel('Métrica')\n",
    "ax.set_ylabel('Valores')\n",
    "ax.set_title('Validação - Precisão, Acurácia e ROC-AUC dos Modelos para os Títulos (Com Zoom)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Precisão', 'Acurácia', 'ROC-AUC'])\n",
    "ax.legend()\n",
    "\n",
    "# Setting y-axis limit for zoom\n",
    "plt.ylim(0.7, 1)\n",
    "\n",
    "# Displaying the plot with zoom\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
